{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Scraper  \n",
    "\n",
    "### Overview  \n",
    "The **X Scraper** is a web scraping tool designed to automatically log into the X platform (formerly known as Twitter) and extract posts based on specific hashtags or keywords. Utilizing the Selenium WebDriver, this scraper navigates the X website, interacts with the user interface, and collects relevant data from posts for analysis.  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "#### Workflow Overview  \n",
    "\n",
    "1. **Driver Initialization**:  \n",
    "   - The scraper initializes a Selenium WebDriver instance configured with Chrome.   \n",
    "   - It can run in headless mode (without a graphical user interface) for efficiency and can be customized for different screen sizes.  \n",
    "\n",
    "2. **Login Process**:  \n",
    "   - The scraper navigates to the X login page and enters the provided username and password.  \n",
    "   - It waits for the necessary elements to load and clicks through the login process, ensuring a successful login.  \n",
    "\n",
    "3. **Tag Scraping**:  \n",
    "   - Once logged in, the scraper iterates through a list of specified hashtags or keywords.  \n",
    "   - For each tag, it clears the search box, enters the tag, and navigates to the \"Latest\" tab to view **recent** posts related to the tag.  \n",
    "\n",
    "4. **Data Extraction**:  \n",
    "   - The scraper scrolls through the loaded posts, extracting relevant data from each post, including:  \n",
    "     - **Username**: The name of the user who made the post.  \n",
    "     - **Time**: The timestamp of the post in a formatted string.  \n",
    "     - **Content**: The text content of the post.  \n",
    "\n",
    "5. **Handling Duplicates**:  \n",
    "   - To avoid processing the same post multiple times, the scraper keeps track of seen posts using their URLs.  \n",
    "\n",
    "6. **Data Storage**:  \n",
    "   - Extracted data is stored in a structured format (list of dictionaries) for further analysis.  \n",
    "   - After scraping, the data is saved to an Excel file for easy access and analysis.  \n",
    "\n",
    "7. **Optional Features**:  \n",
    "   - The scraper includes commented-out sections for saving data in YAML format and uploading files to AWS S3 for cloud storage.  \n",
    "\n",
    "#### Example of Extracted Data  \n",
    "\n",
    "The scraper collects the following types of data for each post:  \n",
    "\n",
    "| Field          | Description                              |  \n",
    "|----------------|------------------------------------------|  \n",
    "| Post Number    | A unique number for the post            |  \n",
    "| Tag            | The hashtag or keyword being discussed   |  \n",
    "| Username       | The name of the user who made the post   |  \n",
    "| Time           | The formatted time of the post           |  \n",
    "| Date           | The date of extraction (YYYY-MM-DD)     |  \n",
    "| Content        | The actual content of the post           |  \n",
    "\n",
    "### Use Cases  \n",
    "\n",
    "The data extracted by the X Scraper can be utilized for various purposes, including:  \n",
    "\n",
    "- **Social Media Analysis**: Analyze trends and sentiments around specific topics or events.  \n",
    "- **Market Research**: Gather insights into public opinion regarding products or brands.  \n",
    "- **Content Monitoring**: Track discussions and feedback related to specific hashtags or campaigns.  \n",
    "\n",
    "### Important Notes  \n",
    "\n",
    "- **Temporary Accounts**: It is recommended to use temporary accounts for scraping to avoid potential violations of X's terms of service. Be cautious when using genuine accounts.  \n",
    "- **Dynamic Content**: The scraper relies on specific XPath and CSS selectors to extract data. Changes in the X platform's layout may require updates to these selectors.  \n",
    "- **AWS S3 Configuration**: The scraper has provisions for uploading extracted data to AWS S3, which can be uncommented and configured as needed.  \n",
    "\n",
    "### Conclusion  \n",
    "\n",
    "The X Scraper provides an efficient way to gather and analyze social media data from the X platform. By automating the login and data extraction processes, it enables users to gain valuable insights into trends and discussions in a timely manner.   \n",
    "\n",
    "\n",
    "**Important Note**: A temporary account has been created for this scraping task. Please be cautious when using genuine accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 Configuration (uncomment and fill in your credentials)\n",
    "# AWS_ACCESS_KEY = 'YOUR_AWS_ACCESS_KEY'\n",
    "# AWS_SECRET_KEY = 'YOUR_AWS_SECRET_KEY'\n",
    "# BUCKET_NAME = 'YOUR_BUCKET_NAME'\n",
    "# s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver with or wihtout Headless Mode\n",
    "def init_driver(chromedriver_path):\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    chrome_options.add_argument(\"--disable-web-security\")  # Disable web security \n",
    "    chrome_options.add_argument(\"--window-size=1280,1024\")\n",
    "    chrome_options.add_argument(\"--ignore-certificate-errors\")  # Ignore SSL errors  \n",
    "    chrome_options.add_argument(\"--allow-insecure-localhost\")  # Allow insecure localhost connections\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver_path, options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    print(\"driver intiated successfuly\")\n",
    "    return driver, wait\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = r\"D:\\coding\\freelancing\\stockMarket\\chromedriver.exe\"\n",
    "driver, wait = init_driver(chromedriver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_x(username, password):\n",
    "\n",
    "    try:\n",
    "        # Navigate to X login page\n",
    "        driver.get(\"https://x.com/login/\")\n",
    "        time.sleep(5)\n",
    "        username_input = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete='username']\")))\n",
    "        username_input.click()\n",
    "        username_input.send_keys(username)\n",
    "\n",
    "        # Click the \"Next\" button\n",
    "        next_button = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[.//span[text()='Next']]\")))\n",
    "        next_button.click()\n",
    "\n",
    "        # Wait for the password input box and enter the password\n",
    "        password_input = wait.until(\n",
    "            EC.visibility_of_element_located((By.XPATH, \"//input[@name='password']\")))\n",
    "\n",
    "        password_input.click()\n",
    "        password_input.send_keys(password)\n",
    "\n",
    "        # Wait for the login button to be clickable and click it\n",
    "        final_login_button =  wait.until(  \n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[@data-testid='LoginForm_Login_Button']\"))  )  \n",
    "        final_login_button.click()\n",
    "        # Verify login was successful (optional)\n",
    "        time.sleep(5)  # Allow some time for the home page to load\n",
    "        print(\"Login successful!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during login: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"Mary01909527219\"\n",
    "password = \"maria33221\"\n",
    "\n",
    "# Log in to X\n",
    "login_to_x(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_search_box(driver, wait):\n",
    "    \"\"\"\n",
    "    Function to clear and refocus on the search box safely.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Re-locate the search box dynamically\n",
    "        search_box = wait.until(\n",
    "            EC.visibility_of_element_located((By.XPATH, \"//input[@data-testid='SearchBox_Search_Input' and @placeholder='Search']\"))\n",
    "        )\n",
    "        \n",
    "        # Clear the value using JavaScript\n",
    "        driver.execute_script(\"arguments[0].value = '';\", search_box)\n",
    "        \n",
    "        # Ensure the search box is focused for further interaction\n",
    "        search_box.click()\n",
    "        \n",
    "        return search_box  # Return the cleared search box\n",
    "    except Exception as e:\n",
    "        print(f\"Error while clearing search box: {e}\")\n",
    "        return None\n",
    "\n",
    "def search_tag(driver, wait, tag):\n",
    "    \"\"\"\n",
    "    Search for the tag in the search box and click on the \"Latest\" tab.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clear and focus on the search box\n",
    "        search_box = clear_search_box(driver, wait)\n",
    "        if not search_box:\n",
    "            print(f\"Failed to locate search box for tag: {tag}\")\n",
    "            return  # Skip this tag if search box is unavailable\n",
    "\n",
    "        # Type the new tag\n",
    "        search_box.send_keys(tag)\n",
    "        search_box.send_keys(Keys.RETURN)  # Start search\n",
    "\n",
    "        # Wait for the \"Latest\" tab to be visible and click it\n",
    "        tab_list = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div[role='tablist']\")))\n",
    "        latest_tab = tab_list.find_elements(By.TAG_NAME, 'a')[1]  # Select \"Latest\" tab\n",
    "        latest_tab.click()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while searching for tag '{tag}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_x(driver, wait, tags, scroll_count, max_posts_per_tag):\n",
    "    \"\"\"\n",
    "    Scrape X (Twitter) posts and comments for a list of tags/tickers.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        wait: WebDriverWait instance.\n",
    "        tags: List of tags or tickers to scrape.\n",
    "        scroll_count: Number of scrolls to load more posts.\n",
    "        max_posts_per_tag: Maximum number of posts to scrape per tag.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing post data.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    post_number = 1  # Initialize post number globally\n",
    "\n",
    "    for tag in tags:\n",
    "        print(f\"Scraping posts for: {tag}\")\n",
    "        try:\n",
    "            # Navigate to the tag page and perform the search\n",
    "            driver.get(f\"https://x.com\")\n",
    "            time.sleep(5)  # Allow page to load\n",
    "\n",
    "            # Perform the search and click on the \"Latest\" tab\n",
    "            search_tag(driver, wait, tag)\n",
    "\n",
    "            total_scraped_posts = 0  # Track total posts scraped for this tag\n",
    "            post_links = set()  # Track links to avoid duplicates\n",
    "\n",
    "            # Capture post links from the first scroll (first batch of posts)\n",
    "            for scroll in range(scroll_count):\n",
    "                if total_scraped_posts >= max_posts_per_tag:\n",
    "                    break  # Stop scrolling if max posts per tag reached\n",
    "\n",
    "                print(f\"Scrolling {scroll + 1}/{scroll_count} for: {tag}\")\n",
    "\n",
    "                # Wait for the post links to load\n",
    "                wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//a[contains(@href, '/status/')]\")))\n",
    "\n",
    "                # Capture post links (Re-fetch after scrolling)\n",
    "                post_links_elements = driver.find_elements(By.XPATH, \"//a[contains(@href, '/status/')]\")\n",
    "                for link in post_links_elements:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    # Only include links that match the specified pattern\n",
    "                    if href and \"/status/\" in href and not any(ends in href for ends in (\n",
    "                        \"/photo\", \"/analytics\", \"/media_tags\")):\n",
    "                        post_links.add(href)  # Add links to the set\n",
    "\n",
    "                print(f\"Captured {len(post_links)} post links.\")\n",
    "\n",
    "                # Scroll for more posts (if needed)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(3)  # Allow time for the page to load more posts\n",
    "\n",
    "            # Scrape each post sequentially using captured links\n",
    "            for link in list(post_links)[:max_posts_per_tag]:\n",
    "                try:\n",
    "                    print(f\"Scraping post from link: {link}\")\n",
    "                    # Navigate to the post\n",
    "                    driver.get(link)\n",
    "                    time.sleep(2)  # Allow time for the post to load\n",
    "\n",
    "                    # Wait for the specific post content element\n",
    "                    post_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-testid='primaryColumn'] .css-175oi2r h2 span\")))\n",
    "\n",
    "                    # Extract username\n",
    "                    username = \"N/A\"\n",
    "                    try:\n",
    "                        username_element = wait.until(EC.presence_of_element_located(\n",
    "                            (By.XPATH, \"//div[@data-testid='User-Name']//span[starts-with(text(), '@')]\")))\n",
    "                        username = username_element.text\n",
    "                    except Exception:\n",
    "                        username = \"N/A\"\n",
    "\n",
    "                    # Extract time and date posted\n",
    "                    date_posted, time_posted = \"N/A\", \"N/A\"\n",
    "                    try:\n",
    "                        time_element = wait.until(EC.presence_of_element_located(\n",
    "                            (By.XPATH, \"//a[contains(@href, '/status/') and @role='link']\")))\n",
    "                        datetime_value = time_element.find_element(By.TAG_NAME, \"time\").get_attribute(\"datetime\")\n",
    "                        date_posted, time_posted = datetime_value.split('T')\n",
    "                        time_posted = time_posted[:-1]  # Remove 'Z'\n",
    "                    except Exception:\n",
    "                        date_posted, time_posted = \"N/A\", \"N/A\"\n",
    "\n",
    "                    # Extract message content\n",
    "                    message_content = \"N/A\"\n",
    "                    try:\n",
    "                        post_content_element = driver.find_element(By.XPATH, \"//div[contains(@data-testid, 'tweetText')]\")\n",
    "                        message_content = post_content_element.text\n",
    "                    except Exception:\n",
    "                        message_content = \"N/A\"\n",
    "\n",
    "                    # Extract comments\n",
    "                    comments = []\n",
    "                    try:\n",
    "                        comments = driver.execute_script(\"\"\"\n",
    "                            let commentsArray = [];\n",
    "                            const commentElements = document.querySelectorAll(\"div[data-testid='tweetText'] span\");\n",
    "                            commentElements.forEach(el => {\n",
    "                                const text = el.innerText.trim();\n",
    "                                if (text && text.length > 1 && !text.startsWith(\"#\")) {\n",
    "                                    commentsArray.push(text);\n",
    "                                }\n",
    "                            });\n",
    "                            return commentsArray;\n",
    "                        \"\"\")\n",
    "                    except Exception:\n",
    "                        comments = []\n",
    "\n",
    "                    # Remove message_content from comments if present  \n",
    "                    # if isinstance(comments, str):  \n",
    "                    #     cleaned_comments = comments.replace(message_content, '').strip()  \n",
    "                    # else:  \n",
    "                    #     cleaned_comments = comments\n",
    "\n",
    "                    # Append data\n",
    "                    all_data.append({\n",
    "                        'Post Number': post_number,\n",
    "                        'label': \"x_data\",\n",
    "                        'Tag': tag,\n",
    "                        'Username': username,\n",
    "                        'Time Posted': time_posted,\n",
    "                        'Date Posted': date_posted,\n",
    "                        'Content': message_content,\n",
    "                        'Comments': comments,\n",
    "                    })\n",
    "\n",
    "                    post_number += 1  # Increment post number\n",
    "                    total_scraped_posts += 1  # Increment total scraped posts\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping post from link {link} due to an error: {e}\")\n",
    "                    continue  # Skip to the next iteration\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag {tag}: {e}\")\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_and_upload_data(data, bucket_name):\n",
    "def save_and_upload_data(data):\n",
    "    \"\"\"\n",
    "    Save data to Excel and YAML and upload to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        data: The data to save and upload.\n",
    "        bucket_name: The name of the S3 bucket to upload to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save to YAML with UTF-8 encoding\n",
    "        yaml_filename = f\"x_data_{datetime.now().strftime('%Y-%m-%d')}.yaml\"\n",
    "        with open(yaml_filename, 'w', encoding='utf-8') as yaml_file:\n",
    "            yaml.dump(data, yaml_file, allow_unicode=True)\n",
    "        print(f\"Data saved to YAML: {yaml_filename}\")\n",
    "\n",
    "        # Save to Excel (if needed, implement this part)\n",
    "        # excel_filename = f\"stocktwits_data_{datetime.now().strftime('%Y-%m-%d')}.xlsx\"\n",
    "        # Implement Excel saving logic here using pandas or openpyxl\n",
    "\n",
    "        # Upload to S3\n",
    "        # s3 = boto3.client('s3')\n",
    "        # # s3.upload_file(excel_filename, bucket_name, excel_filename)\n",
    "        # s3.upload_file(yaml_filename, bucket_name, yaml_filename)\n",
    "        # print(f\"Files uploaded to S3 bucket: {bucket_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or uploading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping posts for: #tesla\n",
      "Scrolling 1/1 for: #tesla\n",
      "Captured 6 post links.\n",
      "Scraping post from link: https://x.com/taki98001/status/1883476742599569755\n",
      "Scraping post from link: https://x.com/hujailgor/status/1883476548193542634\n",
      "Scraping post from link: https://x.com/ElleCoco2/status/1883476839047512067\n",
      "Scraping post from link: https://x.com/NIOSwitzerland/status/1883476956970467448\n",
      "Scraping post from link: https://x.com/VQuaschning/status/1883476498088403395\n",
      "Scraping post from link: https://x.com/mariuskarma/status/1883477247761453416\n",
      "Data saved to YAML: x_data_2025-01-26.yaml\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # chromedriver_path = r\"D:\\coding\\freelancing\\stockMarket\\chromedriver.exe\"\n",
    "    # driver, wait = init_driver(chromedriver_path)\n",
    "    # username = \"Mary01909527219\"\n",
    "    # password = \"maria33221\"\n",
    "\n",
    "\n",
    "    tags = [\"#tesla\"]\n",
    "    try:\n",
    "        # Log in to X\n",
    "        # login_to_x(username, password)\n",
    "        scraped_data = scrape_x(driver, wait, tags, scroll_count=1, max_posts_per_tag=6)\n",
    "        # save_and_upload_data(scraped_data, bucket_name)\n",
    "        save_and_upload_data(scraped_data)\n",
    "    except Exception as e:\n",
    "        print(f\"error in main: {e}\")\n",
    "    # finally:\n",
    "        # driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
