{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocktwits Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocktwits Scraper  \n",
    "\n",
    "### How It Works  \n",
    "\n",
    "The **Stocktwits Scraper** is designed to automate the process of logging into the Stocktwits platform and extracting relevant posts based on specific tags or stock tickers. It employs the **Selenium WebDriver** to perform browser automation, allowing it to navigate web pages, wait for elements to load, and extract information from dynamic content.  \n",
    "\n",
    "#### Workflow Overview  \n",
    "\n",
    "1. **Login Process**:   \n",
    "   - The scraper begins by logging into Stocktwits using a provided username and password.   \n",
    "   - It navigates to the login page, enters the user credentials, and clicks the login button.   \n",
    "   - The scraper waits for a successful login by checking for the presence of specific elements on the page.  \n",
    "\n",
    "2. **Navigating to Tags**:  \n",
    "   - After logging in, the scraper iterates through a list of specified tags or stock tickers.  \n",
    "   - For each tag, it constructs the appropriate URL and navigates to the corresponding Stocktwits page.  \n",
    "\n",
    "3. **Scrolling for Posts**:  \n",
    "   - The scraper performs predefined scrolls on the page to load additional posts dynamically.  \n",
    "   - Each scroll action may reveal new content that needs to be captured.  \n",
    "\n",
    "4. **Data Extraction**:  \n",
    "   - For each loaded set of posts, the scraper extracts relevant data, including:  \n",
    "     - **Post Number**: A sequential identifier for each post.  \n",
    "     - **Tag**: The specific stock ticker or tag associated with the post.  \n",
    "     - **Username**: The name of the user who authored the post.  \n",
    "     - **Time Posted**: The timestamp indicating when the post was made, extracted from the relevant HTML element.  \n",
    "     - **Date**: The date of extraction in YYYY-MM-DD format.  \n",
    "     - **Content**: The text body of the post, which may contain comments, discussions, or market insights.  \n",
    "\n",
    "5. **Storing Data**:  \n",
    "   - Extracted data is stored in a structured format, specifically a list of dictionaries, with each dictionary representing a single post.  \n",
    "   - After processing all specified tags, the scraper returns the collected data for further analysis.  \n",
    "\n",
    "#### Example of Extracted Data  \n",
    "\n",
    "The scraper collects the following types of data for each post:  \n",
    "\n",
    "| Field          | Description                              |  \n",
    "|----------------|------------------------------------------|  \n",
    "| Post Number    | A unique number for the post            |  \n",
    "| Tag            | The stock symbol or tag being discussed |  \n",
    "| Username       | The name of the user who made the post  |  \n",
    "| Time Posted     | The timestamp of when the post was submitted  |  \n",
    "| Date           | The extraction date (YY-MM-DD)          |  \n",
    "| Content        | The actual content of the post           |  \n",
    "\n",
    "### Use Cases  \n",
    "\n",
    "The data extracted by the Stocktwits Scraper can be utilized for various purposes, such as:  \n",
    "\n",
    "- **Market Analysis**: Analyze sentiment around specific stocks or market trends.  \n",
    "- **Investor Insights**: Gain perspectives from the community regarding specific investments.  \n",
    "- **Trend Monitoring**: Track discussions around particular topics or events in real-time.  \n",
    "\n",
    "### Conclusion  \n",
    "\n",
    "The Stocktwits Scraper combines automation with data extraction, making it a valuable tool for anyone interested in gaining insights from social media discussions related to stocks. The ability to gather and analyze this data can provide a competitive edge in understanding market sentiment and investor behavior.\n",
    "\n",
    "**Important Note**: A temporary account has been created for this scraping task. Please be cautious when using genuine accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver with or wihtout Headless Mode\n",
    "def init_driver(chromedriver_path):\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--window-size=1280,1024\")\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver_path, options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    print(\"driver intiated successfuly\")\n",
    "    return driver, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = r\"D:\\coding\\freelancing\\stockMarket\\chromedriver.exe\"\n",
    "driver, wait = init_driver(chromedriver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log In to Stocktwits\n",
    "def login_to_stocktwits(username, password):\n",
    "    \"\"\"\n",
    "    Log in to Stocktwits with provided credentials.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://stocktwits.com/signin\")\n",
    "        time.sleep(6)  # Allow the page to load\n",
    "        try:\n",
    "            accept_button = wait.until(\n",
    "            EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "            # Click the \"I Accept\" button\n",
    "            accept_button.click()\n",
    "        except:\n",
    "            print(\"I Accept button not appeared\")\n",
    "        # Wait for the username input field and enter the username\n",
    "        username_input = wait.until(\n",
    "            EC.visibility_of_element_located((By.CSS_SELECTOR, \"input[data-testid='log-in-username']\")))\n",
    "        username_input.send_keys(username)  # Replace with your username\n",
    "\n",
    "        password_input = wait.until(\n",
    "            EC.visibility_of_element_located((By.CSS_SELECTOR, \"input[data-testid='log-in-password']\")))\n",
    "        password_input.send_keys(password)  # Replace with your password\n",
    "\n",
    "\n",
    "        login_button = wait.until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[data-testid='log-in-submit']\")))\n",
    "\n",
    "        login_button.click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        print(\"Login successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Stocktwits login: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials and configuration\n",
    "username = \"your_user\"\n",
    "password = \"pass\"\n",
    "login_to_stocktwits(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stocktwits(driver, wait, tags, scroll_count, max_posts_per_tag, min_posts_per_tag=0):  \n",
    "    \"\"\"  \n",
    "    Scrape Stocktwits posts and comments for a list of tags/tickers.  \n",
    "\n",
    "    Args:  \n",
    "        driver: Selenium WebDriver instance.  \n",
    "        wait: WebDriverWait instance.  \n",
    "        tags: List of tags or tickers to scrape.  \n",
    "        scroll_count: Number of scrolls to load more posts.  \n",
    "        max_posts_per_tag: Maximum number of posts to scrape per tag.  \n",
    "        min_posts_per_tag: Minimum number of posts to scrape per tag.  \n",
    "\n",
    "    Returns:  \n",
    "        List of dictionaries containing post data.  \n",
    "    \"\"\"  \n",
    "    all_data = []  \n",
    "    post_number = 1  # Initialize post number globally  \n",
    "\n",
    "    for tag in tags:  \n",
    "        print(f\"Scraping posts for: {tag}\")  \n",
    "        try:  \n",
    "            # Navigate to the tag page  \n",
    "            driver.get(f\"https://stocktwits.com/symbol/{tag}\")  \n",
    "            time.sleep(5)  # Allow page to load  \n",
    "\n",
    "            total_scraped_posts = 0  # Track total posts scraped for the tag  \n",
    "            post_links = set()  # Use a set to avoid duplicates  \n",
    "\n",
    "            # Capture post links from the first scroll (first batch of posts)  \n",
    "            for scroll in range(scroll_count):  \n",
    "                if total_scraped_posts >= max_posts_per_tag:  \n",
    "                    break  # Stop scrolling if max posts per tag reached  \n",
    "\n",
    "                print(f\"Scrolling {scroll + 1}/{scroll_count} for: {tag}\")  \n",
    "\n",
    "                # Wait for the post links to load  \n",
    "                wait.until(  \n",
    "                    EC.presence_of_all_elements_located((By.XPATH, \"//a[contains(@href, '/message/')]\"))  \n",
    "                )  \n",
    "\n",
    "                # Capture post links  \n",
    "                try:  \n",
    "                    post_links_elements = driver.find_elements(By.XPATH, \"//a[contains(@href, '/message/')]\")  \n",
    "                    for link in post_links_elements:  \n",
    "                        post_links.add(link.get_attribute(\"href\"))  # Add links to the set  \n",
    "                    print(f\"Captured {len(post_links)} post links.\")  \n",
    "                except Exception as e:  \n",
    "                    print(\"An error occurred while capturing post links:\", str(e))  \n",
    "\n",
    "                # Scroll for more posts (if needed)  \n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  \n",
    "                time.sleep(3)  # Allow time for the page to load more posts  \n",
    "\n",
    "            # Scrape each post sequentially using captured links  \n",
    "            for link in list(post_links)[:max_posts_per_tag]:  \n",
    "                try:  \n",
    "                    print(f\"Scraping post from link: {link}\")  # Debug statement  \n",
    "                    # Navigate to the post  \n",
    "                    driver.get(link)  \n",
    "                    time.sleep(2)  # Allow time for the post to load  \n",
    "\n",
    "                    # Wait for post content to load  \n",
    "                    post_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".RichTextMessage_body__4qUeP\")))  \n",
    "\n",
    "                    # Extract username  \n",
    "                    username = \"N/A\"  \n",
    "                    try:   \n",
    "                        username_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,   \n",
    "                            \"span.StreamMessage_username-default__9l3zP.font-semibold.text-lg[aria-label='Username']\")))  \n",
    "\n",
    "                        # Extract the username text  \n",
    "                        username = username_element.text   \n",
    "                    except Exception:  \n",
    "                        username = \"N/A\"  \n",
    "\n",
    "                    # Extract time  \n",
    "                    date_posted, time_posted = \"N/A\", \"N/A\"  \n",
    "                    try:  \n",
    "                        time_element = wait.until(EC.presence_of_element_located(  \n",
    "                            (By.CSS_SELECTOR, \"div[data-testid='message-header'] time[aria-label='Time this message was posted']\")))  \n",
    "                        datetime_value = time_element.get_attribute(\"datetime\")  \n",
    "                        date_posted, time_posted = datetime_value.split('T')  \n",
    "                        time_posted = time_posted[:-1]  # Remove 'Z'  \n",
    "                    except Exception:  \n",
    "                        date_posted, time_posted = \"N/A\", \"N/A\"  \n",
    "\n",
    "                    # Extract message content  \n",
    "                    message_content = \"N/A\"  \n",
    "                    try:  \n",
    "                        post_content_js_path = (  \n",
    "                            \"return document.querySelector('.RichTextMessage_body__4qUeP').innerText;\"  \n",
    "                        )   \n",
    "                        message_content = driver.execute_script(post_content_js_path)   \n",
    "                    except Exception:  \n",
    "                        message_content = \"N/A\"  \n",
    "\n",
    "                    # Extract comments  \n",
    "                    comments = \"N/A\"  \n",
    "                    try:  \n",
    "                        comments_js_path = (  \n",
    "                            \"return Array.from(document.querySelectorAll('.RichTextMessage_body__4qUeP')).filter(el => \"  \n",
    "                            \"el.innerText.includes('@')).map(el => el.innerText.replace(/@\\\\w+\\\\s/, '').trim()).join('\\\\n');\"  \n",
    "                        )  \n",
    "                        comments = driver.execute_script(comments_js_path)  \n",
    "                    except Exception:  \n",
    "                        comments = \"N/A\"  \n",
    "\n",
    "                    # Append data  \n",
    "                    all_data.append({  \n",
    "                        'Post Number': post_number,\n",
    "                        'label': \"stoctwits_data\",\n",
    "                        'Tag': tag,  \n",
    "                        'Username': username,  \n",
    "                        'Time Posted': time_posted,  \n",
    "                        'Date': date_posted,  \n",
    "                        'Content': message_content,  \n",
    "                        'Comments': comments\n",
    "                    })  \n",
    "\n",
    "                    post_number += 1  # Increment post number  \n",
    "                    total_scraped_posts += 1  # Increment total scraped posts  \n",
    "\n",
    "                except Exception as e:  \n",
    "                    print(f\"Error processing post from link {link}: {e}\")  \n",
    "                    continue  # Skip to the next iteration  \n",
    "\n",
    "            # Check if the minimum posts requirement is met  \n",
    "            if total_scraped_posts < min_posts_per_tag:  \n",
    "                print(f\"Warning: Only {total_scraped_posts} posts scraped for tag '{tag}', which is less than the minimum required ({min_posts_per_tag}).\")  \n",
    "\n",
    "        except Exception as e:  \n",
    "            print(f\"Error processing tag {tag}: {e}\")  \n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_and_upload_data(data, bucket_name):\n",
    "def save_and_upload_data(data):\n",
    "    \"\"\"\n",
    "    Save data to Excel and YAML and upload to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        data: The data to save and upload.\n",
    "        bucket_name: The name of the S3 bucket to upload to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save to YAML with UTF-8 encoding\n",
    "        yaml_filename = f\"stocktwits_data_{datetime.now().strftime('%Y-%m-%d')}.yaml\"\n",
    "        with open(yaml_filename, 'w', encoding='utf-8') as yaml_file:\n",
    "            yaml.dump(data, yaml_file, allow_unicode=True)\n",
    "        print(f\"Data saved to YAML: {yaml_filename}\")\n",
    "\n",
    "        # Save to Excel (if needed, implement this part)\n",
    "        # excel_filename = f\"stocktwits_data_{datetime.now().strftime('%Y-%m-%d')}.xlsx\"\n",
    "        # Implement Excel saving logic here using pandas or openpyxl\n",
    "\n",
    "        # Upload to S3\n",
    "        # s3 = boto3.client('s3')\n",
    "        # # s3.upload_file(excel_filename, bucket_name, excel_filename)\n",
    "        # s3.upload_file(yaml_filename, bucket_name, yaml_filename)\n",
    "        # print(f\"Files uploaded to S3 bucket: {bucket_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or uploading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping posts for: CRSP\n",
      "Scrolling 1/1 for: CRSP\n",
      "Captured 23 post links.\n",
      "Scraping post from link: https://stocktwits.com/audiopile/message/601267669\n",
      "Scraping post from link: https://stocktwits.com/Jupilerke/message/601322936\n",
      "Data saved to YAML: stocktwits_data_2025-01-25.yaml\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # chromedriver_path = r\"path_to_your_chromedriver\"\n",
    "    # driver, wait = init_driver(chromedriver_path)\n",
    "    # bucket_name = \"your-s3-bucket-name\"\n",
    "    # tags = [\"WISH.TSXV\", \"SNDL\", \"CLOV\", \"CRSP\", \"RCPT\", AVAV\", \"ZOM\", \"PLBY\"]\n",
    "    tags = [\"CRSP\"]\n",
    "    try:\n",
    "        # Scrape Stocktwits\n",
    "        scraped_data = scrape_stocktwits(driver, wait, tags, scroll_count=1, max_posts_per_tag=2)\n",
    "        # save_and_upload_data(scraped_data, bucket_name)\n",
    "        save_and_upload_data(scraped_data)\n",
    "    except Exception as e:\n",
    "        print(f\"error in main: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
