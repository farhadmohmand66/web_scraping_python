{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraper  \n",
    "\n",
    "## Overview  \n",
    "The **Reddit Scraper** is a web scraping tool designed to automate the process of logging into Reddit and extracting posts based on specific tags. Utilizing the Selenium WebDriver, this scraper navigates the Reddit website, interacts with the user interface, and collects relevant data from posts for analysis.  \n",
    "\n",
    "## How It Works  \n",
    "\n",
    "### Workflow Overview  \n",
    "\n",
    "1. **Driver Initialization**:  \n",
    "   - The scraper initializes a Selenium WebDriver instance configured to run in headless mode (without a graphical user interface).  \n",
    "   - It sets up necessary options for Chrome, including disabling GPU acceleration and configuring the browser window size.  \n",
    "\n",
    "2. **Login Process**:  \n",
    "   - The scraper navigates to the Reddit login page and enters the provided username and password.  \n",
    "   - It handles the login button, which is located within a Shadow DOM, using JavaScript to ensure successful interaction.  \n",
    "   - After clicking the login button, it checks the current URL to confirm a successful login.  \n",
    "\n",
    "3. **Scraping Posts**:  \n",
    "   - The scraper iterates through a list of specified tags.  \n",
    "   - For each tag, it navigates to the corresponding Reddit search results page and sets the sorting option to \"New\" to get the latest posts.  \n",
    "\n",
    "4. **Data Extraction**:  \n",
    "   - The scraper scrolls through the search results, extracting relevant data from each post, including:  \n",
    "     - **Username**: The name of the user who made the post.  \n",
    "     - **Time Posted**: The timestamp of the post in ISO format.  \n",
    "     - **Content**: The text content of the post.  \n",
    "   - It collects data for a specified number of scrolls per tag.  \n",
    "\n",
    "5. **Navigating Through Posts**:  \n",
    "   - After extracting data from a set of posts, the scraper scrolls down to load more posts dynamically, continuing the extraction process.  \n",
    "\n",
    "6. **Data Storage**:  \n",
    "   - Extracted data is stored in a structured format (list of dictionaries) for further analysis.  \n",
    "\n",
    "### Example of Extracted Data  \n",
    "\n",
    "The scraper collects the following types of data for each post:  \n",
    "\n",
    "| Field          | Description                              |  \n",
    "|----------------|------------------------------------------|  \n",
    "| Post Number    | A unique number for the post            |  \n",
    "| tag    | The tag used to search on Reddit       |  \n",
    "| Username       | The name of the user who made the post   |  \n",
    "| Time Posted    | The timestamp of when the post was submitted |  \n",
    "| Date           | The date of extraction (YYYY-MM-DD)     |  \n",
    "| Content        | The actual content of the post           |  \n",
    "\n",
    "## Use Cases  \n",
    "\n",
    "The data extracted by the Reddit Scraper can be utilized for various purposes, including:  \n",
    "\n",
    "- **Social Media Analysis**: Analyze trends and sentiments around specific topics or communities.  \n",
    "- **Market Research**: Gather insights into public opinion regarding products or services discussed on Reddit.  \n",
    "- **Content Monitoring**: Track discussions and feedback related to specific topics or events.  \n",
    "\n",
    "## Important Notes  \n",
    "\n",
    "- **Temporary Accounts**: It is recommended to use temporary accounts for scraping to avoid potential violations of Reddit's tags of service.  \n",
    "- **Dynamic Content**: The scraper relies on specific XPath and CSS selectors to extract data. Changes in Reddit's layout may require updates to these selectors.  \n",
    "- **Shadow DOM Handling**: The scraper includes methods to interact with elements within the Shadow DOM, which may not be accessible through traditional methods.  \n",
    "\n",
    "## Conclusion  \n",
    "\n",
    "The Reddit Scraper provides an efficient way to gather and analyze social media data from the Reddit platform. By automating the login and data extraction processes, it enables users to gain valuable insights into trends and discussions in a timely manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver with or wihtout Headless Mode\n",
    "def init_driver(chromedriver_path):\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    chrome_options.add_argument(\"--disable-web-security\")  # Disable web security \n",
    "    chrome_options.add_argument(\"--window-size=1280,1024\")\n",
    "    chrome_options.add_argument(\"--ignore-certificate-errors\")  # Ignore SSL errors  \n",
    "    chrome_options.add_argument(\"--allow-insecure-localhost\")  # Allow insecure localhost connections\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver_path, options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    print(\"driver intiated successfuly\")\n",
    "    return driver, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toshiba\\AppData\\Local\\Temp\\ipykernel_16588\\2635080079.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chromedriver_path, options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver intiated successfuly\n"
     ]
    }
   ],
   "source": [
    "chromedriver_path = r\"D:\\coding\\freelancing\\stockMarket\\chromedriver.exe\"\n",
    "driver, wait = init_driver(chromedriver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log In to Reddit\n",
    "def login_to_reddit(driver, wait, username, password):\n",
    "    \"\"\"\n",
    "    Log in to Reddit with provided credentials.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://www.reddit.com/login/\")\n",
    "        time.sleep(5)\n",
    "        # Enter username\n",
    "        username_input = wait.until(EC.presence_of_element_located((By.ID, \"login-username\")))\n",
    "        username_input.send_keys(username)\n",
    "\n",
    "        # Enter password\n",
    "        password_input = wait.until(EC.presence_of_element_located((By.ID, \"login-password\")))\n",
    "        password_input.send_keys(password)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "        # Locate the login button inside the Shadow DOM using JavaScript\n",
    "            login_button = driver.execute_script(\"\"\"\n",
    "                return document\n",
    "                    .querySelector(\"body > shreddit-app > shreddit-overlay-display\")\n",
    "                    .shadowRoot.querySelector(\"shreddit-signup-drawer\")\n",
    "                    .shadowRoot.querySelector(\"shreddit-drawer > div > shreddit-async-loader > div > shreddit-slotter\")\n",
    "                    .shadowRoot.querySelector(\"#login > auth-flow-modal > div.w-100 > faceplate-tracker > button\");\n",
    "            \"\"\")\n",
    "\n",
    "            if login_button:\n",
    "                # Click the login button\n",
    "                driver.execute_script(\"arguments[0].click();\", login_button)\n",
    "                print(\"Login button clicked successfully!\")\n",
    "        except Exception:\n",
    "            # Fallback to JavaScript click if standard click fails\n",
    "            login_button_js = driver.find_element(By.XPATH, '//*[@id=\"login\"]/auth-flow-modal/div[2]/faceplate-tracker/button/span/span')\n",
    "            driver.execute_script(\"arguments[0].click();\", login_button_js)\n",
    "            print(\"Login button clicked using JavaScript!\")\n",
    "\n",
    "            # Wait for successful login\n",
    "            time.sleep(5)\n",
    "            if \"login\" in driver.current_url.lower():\n",
    "                raise Exception(\"Login failed. Still on login page.\")\n",
    "                print(\"Login successful!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Reddit login: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login button clicked using JavaScript!\n",
      "Error during Reddit login: Login failed. Still on login page.\n"
     ]
    }
   ],
   "source": [
    "# Credentials and configuration\n",
    "username = \"uname\"\n",
    "password = \"pass\"\n",
    "login_to_reddit(driver, wait, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddit_post_links(driver, wait, subreddit, scroll_count):\n",
    "    \"\"\"\n",
    "    Extract post links from a specific subreddit.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        wait: WebDriverWait instance.\n",
    "        subreddit: The subreddit to scrape.\n",
    "        scroll_count: Number of scrolls to perform for the subreddit.\n",
    "\n",
    "    Returns:\n",
    "        List of post links.\n",
    "    \"\"\"\n",
    "    post_links = []\n",
    "\n",
    "    print(f\"Extracting post links for subreddit: {subreddit}\")\n",
    "    driver.get(f\"https://www.reddit.com/r/{subreddit}/\")\n",
    "    time.sleep(5)  # Allow page to load\n",
    "\n",
    "    for scroll in range(scroll_count):\n",
    "        print(f\"Scrolling {scroll + 1}/{scroll_count} for subreddit: {subreddit}\")\n",
    "\n",
    "        # Loop through posts using index numbers\n",
    "        for post_index in range(1, 3):  # Adjust range as needed\n",
    "            try:\n",
    "                # Locate the post using the index\n",
    "                post_element = wait.until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (By.XPATH, f\"(//shreddit-post)[{post_index}]\")\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Extract the post link\n",
    "                link_element = post_element.find_element(By.XPATH, \".//a[contains(@href, '/r/') and @slot='full-post-link']\")\n",
    "                post_link = link_element.get_attribute(\"href\")\n",
    "\n",
    "                # Append the link to the list\n",
    "                post_links.append(post_link)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post index {post_index}: {e}\")\n",
    "\n",
    "        # Scroll for more posts\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "\n",
    "    print(f\"Captured {len(post_links)} links for subreddit: {subreddit}\")\n",
    "    return post_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_data(driver, wait, subreddit):\n",
    "    \"\"\"\n",
    "    Extract post data from a specific subreddit.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        wait: WebDriverWait instance.\n",
    "        subreddit: The subreddit to scrape.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries containing post data.\n",
    "    \"\"\"\n",
    "    post_data = []\n",
    " # addjust scrol cound according your requirements\n",
    "    subreddit_links = extract_reddit_post_links(driver, wait, subreddit, scroll_count=1)\n",
    "\n",
    "    for index, link in enumerate(subreddit_links):\n",
    "        print(f\"Extracting data for post {index + 1}: {link}\")\n",
    "        data = {\n",
    "            'Post Number': str(index + 1),\n",
    "            'Label': 'reddit_data',\n",
    "            'Sub-reddit': subreddit,\n",
    "            'Username': 'N/A',\n",
    "            'Time Posted': 'N/A',\n",
    "            'Date Posted': 'N/A',\n",
    "            'Content': 'N/A',\n",
    "            'Comments': []\n",
    "        }\n",
    "\n",
    "        # Extract the user\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            user_element = wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.XPATH, \"//a[contains(@class, 'author-name')]\")\n",
    "                )\n",
    "            )\n",
    "            data['Username'] = user_element.text\n",
    "        except Exception as e:\n",
    "            print(\"Error extracting username: N/A\")\n",
    "\n",
    "        # Extract the datetime value for date and time posted\n",
    "        try:\n",
    "            time_element = wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.XPATH, \"//faceplate-timeago/time\")\n",
    "                )\n",
    "            )\n",
    "            datetime_value = time_element.get_attribute(\"datetime\")\n",
    "            data['Date Posted'] = datetime_value.split(\"T\")[0]  # YYYY-MM-DD\n",
    "            data['Time Posted'] = datetime_value.split(\"T\")[1].split(\"+\")[0]  # HH:MM:SS\n",
    "        except Exception as e:\n",
    "            print(\"Error extracting date and time: N/A\")\n",
    "\n",
    "        # Locate the post title element\n",
    "        try:\n",
    "            title_element = wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.XPATH, \"//h1[contains(@class, 'font-semibold') and @slot='title']\")\n",
    "                )\n",
    "            )\n",
    "            data['Content'] = title_element.text.strip()  # Strip to remove any extra whitespace\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting post title\")\n",
    "\n",
    "        # Extract comments\n",
    "        try:\n",
    "            comments_elements = driver.find_elements(By.XPATH, '//div[@class=\"md text-14 rounded-[8px] pb-2xs overflow-hidden\"]//div[@id=\"-post-rtjson-content\"]/p')\n",
    "            data['Comments'] = [comment.text for comment in comments_elements]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred extracting comments\")\n",
    "\n",
    "        post_data.append(data)\n",
    "\n",
    "    return post_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_yaml(data, base_filename):  \n",
    "    \"\"\"  \n",
    "    Save extracted data to a YAML file with the current date in the filename.  \n",
    "\n",
    "    Args:  \n",
    "        data: Data to be saved.  \n",
    "        base_filename: Base name of the YAML file (without extension).  \n",
    "    \"\"\"  \n",
    "    # Get the current date in the format YYYY-MM-DD  \n",
    "    date_today= datetime.now().strftime(\"%Y-%m-%d\")  \n",
    "    \n",
    "    # Create the full filename with date  \n",
    "    filename = f\"{base_filename}_{date_today}.yaml\"  \n",
    "    \n",
    "    with open(filename, 'w') as yaml_file:  \n",
    "        yaml.dump(data, yaml_file, default_flow_style=False)  \n",
    "    print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting post links for subreddit: apple\n",
      "Scrolling 1/1 for subreddit: apple\n",
      "Captured 2 links for subreddit: apple\n",
      "Extracting data for post 1: https://www.reddit.com/r/apple/comments/1iijkk2/ios_app_store_apps_with_screenshotreading_malware/\n",
      "Extracting data for post 2: https://www.reddit.com/r/apple/comments/1idy9vf/apple_reports_record_q1_2025_earnings_with_12430/\n",
      "Extracting post links for subreddit: tesla\n",
      "Scrolling 1/1 for subreddit: tesla\n",
      "Captured 2 links for subreddit: tesla\n",
      "Extracting data for post 1: https://www.reddit.com/r/Tesla/comments/1gxe3oj/bedini_sg_energizer_motor/\n",
      "Extracting data for post 2: https://www.reddit.com/r/Tesla/comments/180wt0g/sad_day_teslas_lab_on_long_island_burned_to_the/\n",
      "Data saved to reddit_data_2025-02-06.yaml\n"
     ]
    }
   ],
   "source": [
    "subreddits = [\"apple\", \"tesla\"]\n",
    "all_post_data = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    post_data = extract_post_data(driver, wait, subreddit)\n",
    "    all_post_data.extend(post_data)  # Combine data from all subreddits\n",
    "\n",
    "# Save all data to a single YAML file\n",
    "save_to_yaml(all_post_data, 'reddit_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
